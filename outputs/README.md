# Pipeline Outputs

This directory contains all outputs generated by the pipeline stages.

## Directory Structure

```
outputs/
├── processed_entities/     # Entity JSON files from extraction
│   ├── processes.json
│   ├── threads.json
│   ├── files.json
│   ├── sockets.json
│   ├── cpus.json
│   ├── event_sequences.json
│   ├── extraction_summary.json
│   └── sequence_summary.json
├── graph_stats/           # Graph construction statistics
│   ├── graph_stats.json
│   └── pipeline_summary.json
└── README.md             # This file
```

## File Descriptions

### processed_entities/

#### Entity Files
Each file contains an array of entity objects extracted from the trace:

- **processes.json** - Process entities with PIDs, names, timestamps
- **threads.json** - Thread entities with TIDs, parent PIDs, timestamps
- **files.json** - File resources with paths, access counts, timestamps
- **sockets.json** - Network sockets with addresses, ports, protocols
- **cpus.json** - CPU cores with IDs and event counts
- **event_sequences.json** - EventSequence nodes with complete event streams

#### Summary Files

- **extraction_summary.json** - Statistics about entity extraction
  - Total events processed
  - Entity counts by type
  - Time range of trace
  
- **sequence_summary.json** - Statistics about event sequence building
  - Total sequences created
  - Operations breakdown (read, write, etc.)
  - Average durations per operation
  - Total bytes transferred

### graph_stats/

- **graph_stats.json** - Neo4j graph construction statistics
  - Nodes created by label
  - Relationships created by type
  - Total nodes and relationships
  
- **pipeline_summary.json** - Complete pipeline execution summary
  - Start and end times
  - Duration per stage
  - Trace metadata
  - Performance metrics

## Example: processes.json

```json
[
  {
    "pid": 1234,
    "name": "apache2",
    "start_time": 12345.678,
    "end_time": 12456.789,
    "thread_count": 8,
    "parent_pid": 1
  },
  ...
]
```

## Example: event_sequences.json

```json
[
  {
    "sequence_id": "seq_read_42",
    "operation": "read",
    "start_time": 12345.678,
    "end_time": 12345.698,
    "count": 16,
    "event_stream": [
      {
        "timestamp": 12345.678,
        "syscall": "read",
        "duration": 0.001,
        "return_value": 4096,
        "key_params": {"fd": 3, "count": 4096}
      },
      ...
    ],
    "entity_target": "/var/www/html/index.html",
    "bytes_transferred": 65536,
    "thread_id": 1235,
    "process_id": 1234,
    "duration_ms": 20.5
  },
  ...
]
```

## Usage

These files serve multiple purposes:

1. **Debugging** - Inspect extracted data at each pipeline stage
2. **Validation** - Verify entity extraction accuracy
3. **Analysis** - Analyze data without querying Neo4j
4. **Checkpointing** - Resume pipeline from intermediate stages
5. **Export** - Share processed data independent of graph database

## Data Flow

```
Stage 1: Trace Parsing
    → (in-memory KernelEvent objects)
    
Stage 2: Entity Extraction
    → outputs/processed_entities/*.json
    
Stage 3: Sequence Building
    → outputs/processed_entities/event_sequences.json
    
Stage 4: Graph Construction
    → outputs/graph_stats/*.json
    → Neo4j database
```

## Cleanup

To clear all outputs:
```bash
rm -rf outputs/processed_entities/*
rm -rf outputs/graph_stats/*
```

The pipeline will regenerate all files on the next run.
